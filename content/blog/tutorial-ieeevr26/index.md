---
title: "Ubiq and Ubiq-Genie Tutorial at IEEE VR 2026"
description: "Schedule and instructions for the Ubiq and Ubig-Genie tutorial at IEEE VR 2026."
lead: "Learn how to build your own social XR systems using Ubiq and then integrate AI systems with Ubiq-Genie at the IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR) 2026!"
date: 2026-01-05T09:19:42+01:00
lastmod: 2026-01-05T09:19:42+01:00
draft: false
weight: 50
images: []
contributors: ["Anthony Steed", "Nels Numan", "Ruijun (Phoenix) Sun", "Daniele Giunchi"]
---

{{< alert context="info" >}}

Full sets of slides and video of the tutorial will be available after
IEEE VR 2026.

{{< /alert >}}

## Summary

In this tutorial we will explore how to build a wide variety of collaborative extended reality (XR) systems using the Ubiq software and then extend them to support artificial-intelligence (AI) based processes using Ubiq-Genie. Ubiq is an open source platform. It is designed to be easily extensible. It enables development of applications or systems that would be difficult or time-consuming on commercial systems. Ubiq-Genie allows AI processes to be integrated in a distributed manner. The tutorial will start with basic concepts, but then explore how more complete demonstrations can be built using new features that have been built on Ubiq and Ubiq-Genie in the past couple of years


{{< alert context="info" >}}

For information on how to join the tutorial, please see the **IEEE VR 2026** [tutorial page](https://ieeevr.org/2026/program/tutorials/3). Time and location to be announced.


{{< /alert >}}

## Background

One of the most promising applications of extended reality technologies are their use for remote collaboration. A very wide variety of social extended reality (SXR) applications are now available; from competitive games amongst small numbers of players; through to conference-like setups supporting hundreds of attendees. At the same time, there is a lot of interest in integrating artificial intelligence (AI)-based algorithms into XR scenes. AI systems can control virtual humans, help with scene generation and provide content for interactive scenarios.

Ubiq is an open source tool that allows developers to very quickly build SXR applications. While introductory tutorials have been given in the past, and are available online, this tutorial will explore some of the new capabilities of Ubiq that specifically support researchers and teachers that want to do more than just explore the basics. We will also discuss our Ubiq-Genie tool and demonstrate how it allows AI-based systems to be integrated in a distributed fashion alleviating XR clients from needing to support such systems. 

The tutorial is backed up by extensive online documentation, other explanatory videos and a growing set of more complex example systems.





## Intended Audience and Technical Level

The tutorial will be of value to any student, researcher or professional who wants to develop their own SXR application and is interested in using AI systems. They probably want to get a grounding in what the challenges are in building more complex systems. Some experience with Unity, C# and Python would be useful but not necessary as the scenarios will be walkthroughs of the components and tools rather than explicit code. Any code will be shared in an open repository. There will be additional online video tutorials and documentation if participants want to explore specific features.


## Expected Value

In this tutorial, participants will learn about SXR technologies and the capabilities of the Ubiq and Ubiq-Genie toolkits. We will give a short critique of alternate platforms. 


## Schedule

During the tutorial, the following topics will be covered. 

{ .table-striped }
| Topic                                                   | Start Time                       |
|---------------------------------------------------------|---------------------------------:|
| General Overview of Social XR & Generative AI Platforms (Steed)         |      TBC |
| Overview of Ubiq (Steed)                  |                    TBC+20 |
| Overview of Ubiq-Genie (Sun, Numan & Giunchi)                  |                    TBC+40 |
| Wrap-Up and Q&A (All)                                    |                    TBC+80 |
{ .table-striped }

## Slides

To be added

## Presenter Bios

**Anthony Steed (Prof)** is Head of the Virtual Environments and Computer Graphics group in the Department of Computer Science at University College London. He has over 25 years' experience in developing effective immersive experiences. While his early work focussed on the engineering of displays and software, more recently it has focussed on user engagement in collaborative and telepresent scenarios. He received the IEEE VGTC's 2016 Virtual Reality Technical Achievement Award.  

**Ruijun (Phoenix) Sun** is a second year Ph.D. student with the VECG group at UCL. He received an MEng in Computer Science, also from UCL. His PhD topic focuses on novel guardian systems and safety mechanisms in Virtual Reality. Ruijun's ongoing project is Dynamic VR safety system using object detection, which is based on OpenVR overlay, ubiq-genie and computer vision to provide real-time detection of dynamic obstacles with the ability to run simultaneously with commercial VR applications.

**Nels Numan** is a PhD candidate in the Virtual Environments and Computer Graphics group in the Department of Computer Science at University College London. His research examines interpersonal and human-AI collaboration in mixed reality, with an emphasis on computationally parsing, representing, and transforming spatial environments to support situated action and shared understanding. During his doctoral studies, he has completed research internships at Google, Niantic Labs, and Microsoft Research. He holds BSc and MSc degrees in Computer Science and has served as a web chair for IEEE VR and IEEE ISMAR.

**Daniele Giunchi** is an Assistant Professor in Scene Understanding and Smart Environments at the University of Birmingham and an Honorary Lecturer at University College London. His research sits at the intersection of HCI, extended reality (XR), computer vision, and intelligent systems, with a focus on multimodal (e.g., speech/gesture/gaze) and advanced interaction, and on integrating AI/LLM-driven processes into collaborative XR. He contributes to the open-source Ubiq ecosystem and has worked on AI-supported behavior design and collaboration in social XR.

